{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本： 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import pysnooper\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "from opencc import OpenCC\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "\n",
    "# 取得此預訓練模型所使用的 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "clear_output()\n",
    "print(\"PyTorch 版本：\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小： 21128\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "print(\"字典大小：\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('data/train.txt', 'r', encoding=\"utf-8\") as fp:\n",
    "    # remove first line\n",
    "    line = fp.readline()\n",
    "    line = fp.readline()\n",
    "    need = 6000\n",
    "    cnt = 1\n",
    "    while line and cnt <= need:\n",
    "        data.append(line)\n",
    "        line = fp.readline()\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '9648ddb62b2544c3bc12d7c9fed2a854',\n",
       " '周六晚到卖场听夜场摇滚',\n",
       " '中央路红星开启“爱木”狂购  三伏天已到，比天气更热的是红星美凯龙的第三届鲁班文化节。这个周末，想避暑的，想找童趣的，想听摇滚的，想买家具的，都可以去红星美凯龙中央路商场。  7月18日18:00-21:00，中央路红星爱木联盟的“仲夏夜party”将闪亮登场。既然是派对，当然少不了音乐，届时将有南京知名摇滚乐队现场驻唱，high唱“红苹果”国际潮流音乐。想抢购家具的消费者，可以通过本报专线025-*******提前预约夜购狂欢。凡成功预约报名的，可在活动签到现场获得夜购预约卡集，凭卡集可领取莫斯利安酸奶一箱或价值150元的丁渤验房券1张，礼品数量有限，领完即止。同时，业主凭卡集还可到夜场活动展厅，尊享百款夜购爆款商品。  作为本次国际潮流音乐之夜的全程赞助商，知名板式品牌红苹果联合红星推出“双红惠”活动，回报顾客。7月6日-19日，顾客关注“红苹果家具”微信公众号可抢红包，另外还可享全屋定制家具优惠，更多活动详情可见中央路红星店堂公告。  暑期活动怎能少得了孩子？近日，儿童家具品牌酷漫居进入中央路红星，7月18日-19日10:20-17:00，儿童t台秀、芭蕾舞表演、小丑游戏、儿童分房讲座等活动精彩纷呈。活动期间还另设儿童活动区域，钓鱼、橡皮泥手工、决明子沙池等着小朋友来玩耍。参与活动的小朋友签到还有精美学习套装赠送，数量有限，送完为止。  这个双休日，到中央路红星买家具的大朋友也有惊喜哦！成交价基础上最高直降5%。此外，会员购物积分将前所未有地直接翻10倍！凭会员卡及当日满额订单扣减积分还可兑换刮刮卡，冷藏箱、塔式电风扇、精美脸谱u盘、莫斯利安酸奶、五常大米、咖啡饮品券、非转基因食用油、书籍……100%有奖，3000份好礼等你抢。陈燕飞\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].split('_!_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5355"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = OpenCC('s2t')\n",
    "\n",
    "converted = cc.convert(data[1].split('_!_')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "title = []\n",
    "content = []\n",
    "\n",
    "for i in data:\n",
    "    splited_data = i.split('_!_')\n",
    "    label.append(splited_data[0])\n",
    "    title.append(cc.convert(splited_data[2]))\n",
    "    content.append(cc.convert(splited_data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'title': title, 'content': content, 'label': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>週六晚到賣場聽夜場搖滾</td>\n",
       "      <td>中央路紅星開啓“愛木”狂購  三伏天已到，比天氣更熱的是紅星美凱龍的第三屆魯班文化節。這個週...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>北京老教授泄露，持有山河藥輔節後下跌公告，速速看看！！！</td>\n",
       "      <td>大家好，我是老王，一個地地道道的老股民 ，有過巔峯，也有過低谷，最高時期單月盈利近三百萬 ，...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>張灘鎮積極開展基幹民兵訓練活動</td>\n",
       "      <td>（漢濱網訊）爲了加強基幹民兵隊伍建設，不斷提升民兵隊伍的國防意識和軍事技能。連日來，張灘鎮按...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>倆小夥無證騎摩托，未成年還試圖闖卡！</td>\n",
       "      <td>兩輛摩托車被正在進行交通違法查處的交警攔了下來。其中一輛摩托車的駕駛人還試圖闖卡逃跑，他們又...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>不好意思，你不配做深圳人!_搜狐汽車_搜狐網</td>\n",
       "      <td>兄dei，路怒+地圖炮一起服用，等待你的就不只是交警的行政處罰，還有無數想人肉你的惠州老鐵。...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title  \\\n",
       "0                   週六晚到賣場聽夜場搖滾   \n",
       "1  北京老教授泄露，持有山河藥輔節後下跌公告，速速看看！！！   \n",
       "2               張灘鎮積極開展基幹民兵訓練活動   \n",
       "3            倆小夥無證騎摩托，未成年還試圖闖卡！   \n",
       "4        不好意思，你不配做深圳人!_搜狐汽車_搜狐網   \n",
       "\n",
       "                                             content label  \n",
       "0  中央路紅星開啓“愛木”狂購  三伏天已到，比天氣更熱的是紅星美凱龍的第三屆魯班文化節。這個週...     1  \n",
       "1  大家好，我是老王，一個地地道道的老股民 ，有過巔峯，也有過低谷，最高時期單月盈利近三百萬 ，...     1  \n",
       "2  （漢濱網訊）爲了加強基幹民兵隊伍建設，不斷提升民兵隊伍的國防意識和軍事技能。連日來，張灘鎮按...     0  \n",
       "3  兩輛摩托車被正在進行交通違法查處的交警攔了下來。其中一輛摩托車的駕駛人還試圖闖卡逃跑，他們又...     2  \n",
       "4  兄dei，路怒+地圖炮一起服用，等待你的就不只是交警的行政處罰，還有無數想人肉你的惠州老鐵。...     2  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## label 0 for positive, 1 for neutural, 2 for negative\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5355\n",
      "1    0.498973\n",
      "2    0.393091\n",
      "0    0.107937\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df.to_csv('data/train_tc.csv')\n",
    "print(len(df))\n",
    "print(df.label.value_counts() / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "此 Dataset 每次將 csv 裡的一筆成對句子轉換成 BERT 相容的格式，並回傳 3 個 tensors：\n",
    "- tokens_tensor：兩個句子合併後的索引序列，包含 [CLS] 與 [SEP]\n",
    "- segments_tensor：可以用來識別兩個句子界限的 binary tensor\n",
    "- label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class sentimentDataset(Dataset):\n",
    "    # 讀取處理後的 csv 並初始化參數\n",
    "    def __init__(self, mode, tokenizer, max_len=512):\n",
    "        assert mode in ['train', 'valid', 'test']\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv('data/' + mode + '_tc.csv')\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "#     @pysnooper.snoop()  # 加入以了解所有轉換過程\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train' or self.mode == 'valid':\n",
    "            content_id, title, content, label = self.df.iloc[idx, :].values\n",
    "            label_tensor = torch.tensor(label)\n",
    "            \n",
    "        # 建立 title 的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = ['[CLS]']\n",
    "        tokens_title = self.tokenizer.tokenize(title)\n",
    "        word_pieces += tokens_title + ['[SEP]']\n",
    "        len_title = len(word_pieces)\n",
    "        \n",
    "        # 建立 content 的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        tokens_content = self.tokenizer.tokenize(content)\n",
    "        word_pieces += tokens_content + ['[SEP]']\n",
    "        if len(word_pieces) > self.max_len:\n",
    "            word_pieces = word_pieces[:self.max_len]\n",
    "        len_content = len(word_pieces) - len_title\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將 tiel 包含 [SEP] 的 token 位置設為 0，其他為 1 表示 content\n",
    "        segments_tensor = torch.tensor([0] * len_title + [1] * len_content, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = sentimentDataset('train', tokenizer=tokenizer, max_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[原始文本]\n",
      "句子 1：週六晚到賣場聽夜場搖滾\n",
      "句子 2：中央路紅星開啓“愛木”狂購  三伏天已到，比天氣更熱的是紅星美凱龍的第三屆魯班文化節。這個週末，想避暑的，想找童趣的，想聽搖滾的，想買傢俱的，都可以去紅星美凱龍中央路商場。  7月18日18:00-21:00，中央路紅星愛木聯盟的“仲夏夜party”將閃亮登場。既然是派對，當然少不了音樂，屆時將有南京知名搖滾樂隊現場駐唱，high唱“紅蘋果”國際潮流音樂。想搶購傢俱的消費者，可以通過本報專線025-*******提前預約夜購狂歡。凡成功預約報名的，可在活動簽到現場獲得夜購預約卡集，憑卡集可領取莫斯利安酸奶一箱或價值150元的丁渤驗房券1張，禮品數量有限，領完即止。同時，業主憑卡集還可到夜場活動展廳，尊享百款夜購爆款商品。  作爲本次國際潮流音樂之夜的全程贊助商，知名板式品牌紅蘋果聯合紅星推出“雙紅惠”活動，回報顧客。7月6日-19日，顧客關注“紅蘋果傢俱”微信公衆號可搶紅包，另外還可享全屋定製傢俱優惠，更多活動詳情可見中央路紅星店堂公告。  暑期活動怎能少得了孩子？近日，兒童傢俱品牌酷漫居進入中央路紅星，7月18日-19日10:20-17:00，兒童t臺秀、芭蕾舞表演、小丑遊戲、兒童分房講座等活動精彩紛呈。活動期間還另設兒童活動區域，釣魚、橡皮泥手工、決明子沙池等着小朋友來玩耍。參與活動的小朋友簽到還有精美學習套裝贈送，數量有限，送完爲止。  這個雙休日，到中央路紅星買傢俱的大朋友也有驚喜哦！成交價基礎上最高直降5%。此外，會員購物積分將前所未有地直接翻10倍！憑會員卡及當日滿額訂單扣減積分還可兌換刮刮卡，冷藏箱、塔式電風扇、精美臉譜u盤、莫斯利安酸奶、五常大米、咖啡飲品券、非轉基因食用油、書籍……100%有獎，3000份好禮等你搶。陳燕飛\n",
      "\n",
      "分類  ：1\n",
      "\n",
      "--------------------\n",
      "\n",
      "[Dataset 回傳的 tensors]\n",
      "tokens_tensor  ：tensor([  101,  6867,  1063,  3241,  1168,  6546,  1842,  5481,  1915,  1842,\n",
      "         3015,  4020,   102,   704,  1925,  6662,  5148,  3215,  7274,  1559,\n",
      "          100,  2695,  3312,   100,  4312,  6554,   676,   826,  1921,  2347,\n",
      "         1168,  8024,  3683,  1921,  3706,  3291,  4229,  4638,  3221,  5148,\n",
      "         3215,  5401,  1134,  7983,  4638,  5018,   676,  2234,  7798,  4408,\n",
      "         3152,  1265,  5059,   511,  6857,   943,  6867,  3314,  8024,  2682,\n",
      "         6912,  3264,  4638,  8024,  2682,  2823,  4997,  6637,  4638,  8024,\n",
      "         2682,  5481,  3015,  4020,  4638,  8024,  2682,  6525,   993,   936,\n",
      "         4638,  8024,  6963,  1377,   809,  1343,  5148,  3215,  5401,  1134,\n",
      "         7983,   704,  1925,  6662,  1555,  1842,   511,   128,  3299,  8123,\n",
      "         3189,  8123,   131,  8136,   118,  8128,   131,  8136,  8024,   704,\n",
      "         1925,  6662,  5148,  3215,  2695,  3312,  5474,  4673,  4638,   100,\n",
      "          815,  1909,  1915,  9126,   100,  2200,  7272,   778,  4633,  1842,\n",
      "          511,  3188,  4197,  3221,  3836,  2205,  8024,  4534,  4197,  2208,\n",
      "          679,   749,  7509,  3556,  8024,  2234,  3229,  2200,  3300,  1298,\n",
      "          776,  4761,  1399,  3015,  4020,  3556,  7386,  4412,  1842,  7688,\n",
      "         1548,  8024,  8757,  1548,   100,  5148,  5981,  3362,   100,  1751,\n",
      "         7396,  4060,  3837,  7509,  3556,   511,  2682,  3024,  6554,   993,\n",
      "          936,  4638,  3867,  6527,  5442,  8024,  1377,   809,  6858,  6882,\n",
      "         3315,  1841,  2201,  5221, 11900,   118,   115,   115,   115,   115])\n",
      "\n",
      "segments_tensor：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "label_tensor   ：1\n",
      "\n",
      "--------------------\n",
      "\n",
      "[還原 tokens_tensors]\n",
      "[CLS]週六晚到賣場聽夜場搖滾[SEP]中央路紅星開啓[UNK]愛木[UNK]狂購三伏天已到，比天氣更熱的是紅星美凱龍的第三屆魯班文化節。這個週末，想避暑的，想找童趣的，想聽搖滾的，想買傢俱的，都可以去紅星美凱龍中央路商場。7月18日18:00-21:00，中央路紅星愛木聯盟的[UNK]仲夏夜party[UNK]將閃亮登場。既然是派對，當然少不了音樂，屆時將有南京知名搖滾樂隊現場駐唱，high唱[UNK]紅蘋果[UNK]國際潮流音樂。想搶購傢俱的消費者，可以通過本報專線025-****\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 選擇第一個樣本\n",
    "sample_idx = 0\n",
    "\n",
    "# 將原始文本拿出做比較\n",
    "content_id, title, content, label = trainset.df.iloc[sample_idx, :].values\n",
    "\n",
    "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "# 將 tokens_tensor 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \"\".join(tokens)\n",
    "\n",
    "# 渲染前後差異，毫無反應就是個 print。可以直接看輸出結果\n",
    "print(f\"\"\"[原始文本]\n",
    "句子 1：{title}\n",
    "句子 2：{content}\n",
    "分類  ：{label}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[Dataset 回傳的 tensors]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "segments_tensor：{segments_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[還原 tokens_tensors]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作可以一次回傳一個 mini-batch 的 DataLoader\n",
    "這個 DataLoader 吃我們上面定義的 `sentimentDataset`，\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 這個函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是\n",
    "# 剛剛定義的 `sentimentDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "# 它會對前兩個 tensors 作 zero padding，並產生 masks_tensors\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label = None\n",
    "        \n",
    "    # zero pad 到同一長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
    "BATCH_SIZE = 5\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([20, 200]) \n",
      "tensor([[ 101, 6867, 1063,  ...,  115,  115,  115],\n",
      "        [ 101, 1266,  776,  ...,  671, 7547, 8024],\n",
      "        [ 101, 2484, 4121,  ..., 4708, 3146, 7968],\n",
      "        ...,\n",
      "        [ 101, 6043, 7943,  ..., 2972, 4708,  671],\n",
      "        [ 101,  679, 4500,  ..., 3621, 2130, 1059],\n",
      "        [ 101, 2255, 6205,  ...,    0,    0,    0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([20, 200])\n",
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([20, 200])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([20])\n",
      "tensor([1, 1, 0, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 0, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, masks_tensors, label = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label.shape}\n",
    "{label}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 載入一個可以做中文多分類任務的模型，n_class = 3\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-chinese'\n",
    "NUM_LABELS = 3\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定義一個可以針對特定 DataLoader 取得模型預測結果以及分類準確度的函式\n",
    "\"\"\"\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將 tenosrs 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors, attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 紀錄當前 batch\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "                \n",
    "        if compute_acc:\n",
    "            acc = correct / total\n",
    "            return predictions, acc\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print('device:', device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.1161531279178338\n"
     ]
    }
   ],
   "source": [
    "print('acc:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "整個分類模型的參數量：102269955\n",
      "線性分類器的參數量：2307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "整個分類模型的參數量：{sum(p.numel() for p in model_params)}\n",
    "線性分類器的參數量：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 532.013, acc: 0.894\n",
      "[epoch 2] loss: 304.207, acc: 0.940\n",
      "[epoch 3] loss: 183.202, acc: 0.943\n",
      "Wall time: 25min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 訓練模式\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam Optmizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "        \n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors,\n",
    "                        token_type_ids=segments_tensors,\n",
    "                        attention_mask=masks_tensors,\n",
    "                        labels=labels)\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # 計算準確率\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "    \n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' % (epoch+1, running_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'data/epoch_3_batch_5_all_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRETRAINED_MODEL_NAME = 'bert-base-chinese'\n",
    "NUM_LABELS = 3\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "model.load_state_dict(torch.load('data/epoch_3_batch_5_1000_samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.498973\n",
       "2    0.393091\n",
       "0    0.107937\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.df.label.value_counts() / len(trainset.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "validset = sentimentDataset('valid', tokenizer=tokenizer, max_len=200)\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "validloader = DataLoader(validset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print('device:', device)\n",
    "model = model.to(device)\n",
    "result, acc = get_predictions(model, validloader, compute_acc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.845691382765531\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.492986\n",
       "2    0.420842\n",
       "0    0.086172\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validset.df.label.value_counts() / len(validset.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = validset.df.label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data distribution for valid data\n",
      "0\t86\n",
      "1\t492\n",
      "2\t420\n",
      "true positive for epoch_3_batch_5_1000_samples model:\n",
      "0\t42\n",
      "1\t437\n",
      "2\t365\n"
     ]
    }
   ],
   "source": [
    "# acc 0.845691382765531\n",
    "print('data distribution for valid data')\n",
    "print(f'0\\t{origin.count(0)}')\n",
    "print(f'1\\t{origin.count(1)}')\n",
    "print(f'2\\t{origin.count(2)}')\n",
    "cnt = 0\n",
    "print('true positive for epoch_3_batch_5_1000_samples model:')\n",
    "for i in range(3):\n",
    "    cnt = 0\n",
    "    for o, p in zip(origin, result):\n",
    "        if o==i and p==i:\n",
    "            cnt += 1\n",
    "    print(f'{i}\\t{cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data distribution for valid data\n",
      "0\t86\n",
      "1\t492\n",
      "2\t420\n",
      "true positive for epoch_3_batch_5_all_samples model:\n",
      "0\t67\n",
      "1\t381\n",
      "2\t357\n"
     ]
    }
   ],
   "source": [
    "# acc 0.8066132264529058\n",
    "print('data distribution for valid data')\n",
    "print(f'0\\t{origin.count(0)}')\n",
    "print(f'1\\t{origin.count(1)}')\n",
    "print(f'2\\t{origin.count(2)}')\n",
    "cnt = 0\n",
    "print('true positive for epoch_3_batch_5_all_samples model:')\n",
    "for i in range(3):\n",
    "    cnt = 0\n",
    "    for o, p in zip(origin, result):\n",
    "        if o==i and p==i:\n",
    "            cnt += 1\n",
    "    print(f'{i}\\t{cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
